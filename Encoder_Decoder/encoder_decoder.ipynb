{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "a9WdxAAWCJP6"
      },
      "source": [
        "PYTORCH PRACTICE\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "49_8PwI0CU-s"
      },
      "source": [
        "ENCODER_DECODER_ARCHITECTURE\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "YOAlrmwUoxCr"
      },
      "outputs": [],
      "source": [
        "\n",
        "\n",
        "import torch\n",
        "import transformers\n",
        "from tensorflow.keras.layers import TextVectorization\n",
        "\n",
        "\n",
        "\n",
        "sentences = [\n",
        "    \"The early bird catches the worm and starts its day.\",\n",
        "    \"A gentle rain fell all night long, nourishing the newly planted garden.\",\n",
        "    \"She quickly ran to catch the bus before it drove away without her.\",\n",
        "    \"He baked a large cake for his friend's birthday celebration last weekend.\",\n",
        "    \"The old lighthouse stood tall against the crashing waves of the stormy sea.\",\n",
        "    \"My little brother is learning to ride his new bicycle for the very first time.\",\n",
        "    \"A small, friendly kitten played with a ball of yarn on the wooden floor.\",\n",
        "    \"They watched the dazzling fireworks light up the night sky from their balcony.\",\n",
        "    \"The students listened attentively to the teacher's important lecture.\",\n",
        "    \"I like to read mystery novels on the couch on cold and rainy evenings.\",\n",
        "    \"The cheerful dog wagged its tail with excitement at the sight of its owner.\",\n",
        "    \"She picked a large bunch of beautiful flowers from the community park.\",\n",
        "    \"The quiet library was the perfect place for studying and getting work done.\",\n",
        "    \"He sipped a hot cup of coffee while watching the morning news on TV.\",\n",
        "    \"The autumn leaves painted the entire forest in brilliant shades of yellow and orange.\",\n",
        "    \"My grandmother told us many captivating stories about her childhood adventures.\",\n",
        "    \"A clever fox sneaked into the farm and stole a plump, delicious chicken.\",\n",
        "    \"The talented musician played a beautiful melody on his shiny new guitar.\",\n",
        "    \"She wrote a long and heartfelt letter to her dear friend who lived far away.\",\n",
        "    \"The children giggled with joy as they built a massive sandcastle on the beach.\",\n",
        "    \"He finished his chores and then happily spent the rest of the day playing games.\",\n",
        "    \"The fluffy white clouds drifted lazily across the bright blue sky above us.\",\n",
        "    \"My parents cooked a special dinner to celebrate their anniversary last night.\",\n",
        "    \"The brave firefighter quickly rescued the small cat from the burning building.\",\n",
        "    \"She carefully wrapped the special gift in colorful paper for her best friend.\",\n",
        "    \"The old car rattled and squeaked as it slowly drove down the long, dirt road.\",\n",
        "    \"He happily played a lively game of basketball with his friends after school.\",\n",
        "    \"The bright full moon shone down on the quiet, sleepy town below them.\",\n",
        "    \"My sister and I watched our favorite cartoon show on the sofa.\",\n",
        "    \"The birds sang a sweet morning song from the tall, green trees.\"\n",
        "]\n",
        "\n",
        "spanish_sentences = [\n",
        "    \"Al que madruga, Dios le ayuda, y comienza su día.\",\n",
        "    \"Una suave lluvia cayó toda la noche, nutriendo el jardín recién plantado.\",\n",
        "    \"Ella corrió rápidamente para alcanzar el autobús antes de que se fuera sin ella.\",\n",
        "    \"Él horneó un gran pastel para la celebración de cumpleaños de su amigo el fin de semana pasado.\",\n",
        "    \"El viejo faro se erguía alto contra las olas rompientes del mar tempestuoso.\",\n",
        "    \"Mi hermano pequeño está aprendiendo a montar su nueva bicicleta por primera vez.\",\n",
        "    \"Un pequeño y simpático gatito jugaba con una bola de lana en el suelo de madera.\",\n",
        "    \"Ellos vieron los deslumbrantes fuegos artificiales iluminar el cielo nocturno desde su balcón.\",\n",
        "    \"Los estudiantes escucharon atentamente la importante conferencia del profesor.\",\n",
        "    \"Me gusta leer novelas de misterio en el sofá durante las tardes frías y lluviosas.\",\n",
        "    \"El perro alegre meneó la cola con emoción al ver a su dueño.\",\n",
        "    \"Ella recogió un gran ramo de hermosas flores del parque comunitario.\",\n",
        "    \"La tranquila biblioteca era el lugar perfecto para estudiar y hacer el trabajo.\",\n",
        "    \"Él sorbía una taza de café caliente mientras veía las noticias de la mañana en la televisión.\",\n",
        "    \"Las hojas de otoño pintaron todo el bosque en brillantes tonos de amarillo y naranja.\",\n",
        "    \"Mi abuela nos contó muchas historias cautivadoras sobre sus aventuras de la infancia.\",\n",
        "    \"Un astuto zorro se coló en la granja y robó un pollo gordo y delicioso.\",\n",
        "    \"El talentoso músico tocó una hermosa melodía en su nueva y brillante guitarra.\",\n",
        "    \"Ella escribió una larga y sincera carta a su querida amiga que vivía muy lejos.\",\n",
        "    \"Los niños se reían de alegría mientras construían un enorme castillo de arena en la playa.\",\n",
        "    \"Él terminó sus tareas y luego felizmente pasó el resto del día jugando.\",\n",
        "    \"Las mullidas nubes blancas flotaban perezosamente a través del cielo azul brillante sobre nosotros.\",\n",
        "    \"Mis padres cocinaron una cena especial para celebrar su aniversario anoche.\",\n",
        "    \"El valiente bombero rescató rápidamente al pequeño gato del edificio en llamas.\",\n",
        "    \"Ella envolvió cuidadosamente el regalo especial en papel de colores para su mejor amigo.\",\n",
        "    \"El viejo coche traqueteaba y rechinaba mientras bajaba lentamente por el largo camino de tierra.\",\n",
        "    \"Él jugó felizmente un animado partido de baloncesto con sus amigos después de la escuela.\",\n",
        "    \"La luna llena y brillante iluminó el pueblo tranquilo y dormido debajo de ellos.\",\n",
        "    \"Mi hermana y yo vimos nuestro programa de dibujos animados favorito en el sofá.\",\n",
        "    \"Los pájaros cantaron una dulce canción matutina desde los altos árboles verdes.\"\n",
        "]\n",
        "\n",
        "class TEXT_TO_EMBEDD:\n",
        "  def __init__(self) -> None:\n",
        "    self.tokenizer = transformers.AutoTokenizer.from_pretrained(\"bert-base-uncased\")\n",
        "    special_tokens = {'additional_special_tokens': ['[SOS]', '[EOS]']}\n",
        "    self.tokenizer.add_special_tokens(special_tokens)\n",
        "    self.sos_token_id = self.tokenizer.convert_tokens_to_ids('[SOS]')\n",
        "    self.eos_token_id = self.tokenizer.convert_tokens_to_ids('[EOS]')\n",
        "\n",
        "\n",
        "  def convert(self,input_sentences,output_sentences):\n",
        "\n",
        "    output_with_special = []\n",
        "    for sent in output_sentences:\n",
        "        output_with_special.append(f\"[SOS] {sent} [EOS]\")\n",
        "\n",
        "    encoded_outputs = self.tokenizer(output_with_special,padding = True,truncation = True,return_tensors = 'pt')\n",
        "\n",
        "    encoded_inputs = self.tokenizer(input_sentences,padding = True,truncation=True,return_tensors = 'pt')\n",
        "\n",
        "    input_encoding = encoded_inputs['input_ids']\n",
        "    output_encoding = encoded_outputs['input_ids']\n",
        "\n",
        "\n",
        "    vocab_size = self.tokenizer\n",
        "\n",
        "    input_ids = encoded_inputs[\"input_ids\"]\n",
        "    input_mask = encoded_inputs[\"attention_mask\"]\n",
        "    output_ids = encoded_outputs[\"input_ids\"]\n",
        "    output_mask = encoded_outputs[\"attention_mask\"]\n",
        "\n",
        "    return {\"input_ids\": input_ids,\n",
        "            \"output_ids\": output_ids,\n",
        "            \"vocab_size\": len(self.tokenizer),\n",
        "            \"tokenizer\": self.tokenizer\n",
        "            }\n",
        "\n",
        "\n",
        "# '''/// ___ Encoder ___ \\\\\\'''\n",
        "\n",
        "class Encoder(torch.nn.Module):\n",
        "  def __init__(self,input_dim,num_layers,hidden_dim,dropout,*args,**kwargs):\n",
        "        super().__init__(*args,**kwargs)\n",
        "        self.lstm = torch.nn.LSTM(\n",
        "            input_size=input_dim,\n",
        "            hidden_size=hidden_dim,\n",
        "            num_layers=num_layers,\n",
        "            batch_first=True,\n",
        "            dropout=float(dropout) if num_layers > 1 else 0.0\n",
        "        )\n",
        "\n",
        "  def forward(self,Input_embbed):\n",
        "    output,[hiden_state,cell_state] = self.lstm(Input_embbed)\n",
        "    return output,hiden_state,cell_state\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "class Decoder(torch.nn.Module):\n",
        "  def __init__(self,input_dim,num_layers,dropout,hidden_dim,*args,**kwargs):\n",
        "        super().__init__(*args,**kwargs)\n",
        "        self.lstm = torch.nn.LSTM(input_dim,\n",
        "                                  hidden_dim,\n",
        "                                  num_layers,\n",
        "                                  batch_first = True,\n",
        "                                  dropout = float(dropout) if num_layers>1 else 0.0)\n",
        "\n",
        "\n",
        "  def forward(self,output,input_hidden_state):\n",
        "      o,[H0_t,c0_t] = self.lstm(output,input_hidden_state)\n",
        "      return o,H0_t,c0_t\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "class Encoder_Decoder_Architecture(torch.nn.Module):\n",
        "    def __init__(self, emmbed_dim, enc_layers, enc_hidden, enc_drop,\n",
        "                      dec_layers, dec_hidden, dec_drop,\n",
        "                       vocab_size,*args,**kwargs):\n",
        "      super().__init__(*args,**kwargs)\n",
        "      self.embedding = torch.nn.Embedding(vocab_size,emmbed_dim)\n",
        "\n",
        "      self.encoder = Encoder(emmbed_dim, enc_layers, enc_hidden, enc_drop)\n",
        "      self.decoder = Decoder(emmbed_dim, dec_layers, dec_drop, dec_hidden)\n",
        "      self.layer = torch.nn.Linear(dec_hidden, vocab_size)\n",
        "      self.softmax = torch.nn.Softmax(-1)\n",
        "\n",
        "\n",
        "\n",
        "    def forward(self,Sentence_ids,Sentences_translations_ids):\n",
        "      src_emb = self.embedding(Sentence_ids)\n",
        "      target_emb = self.embedding(Sentences_translations_ids)\n",
        "\n",
        "      encoder_output, Hidden_state_encoder, final_cell_state = self.encoder.forward(src_emb)\n",
        "\n",
        "      decoder_output,Hidden_state_decoder,final_decoder_state = self.decoder.forward(target_emb,(Hidden_state_encoder,final_cell_state))\n",
        "\n",
        "      logits = self.layer(decoder_output)\n",
        "\n",
        "      return logits\n",
        "\n",
        "\n",
        "\n",
        "def train(epochs,sentences,translated_sentences,lr = 0.001):\n",
        "\n",
        "  emb = TEXT_TO_EMBEDD()\n",
        "\n",
        "  data = emb.convert(sentences,translated_sentences)\n",
        "  input_ids     = data[\"input_ids\"]\n",
        "  output_ids    = data[\"output_ids\"]\n",
        "  vocab_size    = data[\"vocab_size\"]\n",
        "  tokenizer     = data[\"tokenizer\"]\n",
        "\n",
        "  decoder_input_ids = output_ids[:,:-1]\n",
        "  decoder_target_ids = output_ids[:,1:]\n",
        "\n",
        "  embedd_dim = 128\n",
        "\n",
        "  seq2seq = Encoder_Decoder_Architecture(embedd_dim,enc_layers = 3,enc_hidden = 128,enc_drop=0.3,\n",
        "                                         dec_layers=3,dec_hidden=128,dec_drop=0.2,\n",
        "                                         vocab_size=vocab_size)\n",
        "  optimizer = torch.optim.Adam(seq2seq.parameters(),lr = lr)\n",
        "  loss_fn = torch.nn.CrossEntropyLoss(ignore_index=0)\n",
        "\n",
        "\n",
        "  for i in range(epochs):\n",
        "    seq2seq.train()\n",
        "    optimizer.zero_grad()\n",
        "\n",
        "    logits = seq2seq(input_ids, decoder_input_ids)\n",
        "    B, T, V = logits.shape\n",
        "    logits_flat = logits.reshape(-1,V)\n",
        "    targets_flat = decoder_target_ids.reshape(-1)\n",
        "\n",
        "\n",
        "    ls = loss_fn(logits_flat, targets_flat)\n",
        "    ls.backward()\n",
        "    torch.nn.utils.clip_grad_norm_(seq2seq.parameters(), 1.0)\n",
        "\n",
        "    optimizer.step()\n",
        "\n",
        "    if(i%100 == 0):\n",
        "        preds = torch.argmax(logits, dim=2)\n",
        "        acc = (preds == decoder_target_ids).float().mean()\n",
        "        print(f\"Epoch:{i} , loss: {ls.item():.4f} , accuracy: {acc.item():.4f}\")\n",
        "\n",
        "  return seq2seq, data[\"tokenizer\"]\n",
        "\n",
        "\n",
        "\n",
        "\n"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "gpuType": "T4",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
