{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "3h5gseALZYQH"
      },
      "outputs": [],
      "source": [
        "import tensorflow as tf\n",
        "# import tensorflow_datasets as tfds\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "from sklearn.linear_model import LinearRegression\n",
        "from sklearn.preprocessing import StandardScaler,OneHotEncoder\n",
        "from sklearn.metrics import roc_curve,confusion_matrix\n",
        "from sklearn.model_selection import train_test_split,RandomizedSearchCV\n",
        "\n",
        "tf.random.set_seed(42)\n",
        "\n",
        "df = pd.read_csv(\"/content/train.csv\")\n",
        "#data preparation\n",
        "def preprocess(df):\n",
        "    df_numeric = df.select_dtypes(include=int)\n",
        "    df_object = df.select_dtypes(exclude= int)\n",
        "    freq = df_object['job'].value_counts(normalize=True).to_dict()\n",
        "    df_object[\"job\"] = df['job'].map(freq)\n",
        "    job = df_object['job']\n",
        "    # object Encoding\n",
        "    Encoder = OneHotEncoder(sparse_output=False)\n",
        "    df_converted = Encoder.fit_transform(df_object.drop('job',axis = 1).values)\n",
        "\n",
        "    df_object = pd.DataFrame(df_converted,index = df_object.index)\n",
        "    df_object[\"job\"] = job\n",
        "    df = pd.concat((df_object,df_numeric),axis=1)\n",
        "    return df\n",
        "df = preprocess(df)\n",
        "\n",
        "from sklearn.utils import class_weight\n",
        "'''Since the values of 1s and 0s are imbalanced\n",
        "  their weights must be increased or decreased '''\n",
        "class_weights = class_weight.compute_class_weight(\n",
        "    class_weight=\"balanced\",\n",
        "    classes=np.unique(df[\"y\"]),\n",
        "    y=df[\"y\"]\n",
        ")\n",
        "class_weight_dict = dict(enumerate(class_weights))\n",
        "\n",
        "\n",
        "def prepare_Data(Dataf):\n",
        "  X = Dataf.drop(\"y\",axis = 1)\n",
        "  y = Dataf[\"y\"]\n",
        "  Xtrain,ytrain = X[:600000],y[:600000]\n",
        "  Xvalid,yvalid = X[600000:],y[600000:]\n",
        "  Xtrain_data = tf.data.Dataset.from_tensor_slices((Xtrain,ytrain)).shuffle(buffer_size=10000).batch(100).prefetch(1)\n",
        "  Xvalid_data = tf.data.Dataset.from_tensor_slices((Xvalid,yvalid)).shuffle(buffer_size=10000).batch(100).prefetch(1)\n",
        "  return Xtrain_data,Xvalid_data\n",
        "\n",
        "\n",
        "#Model making\n",
        "class MyModel(tf.keras.Model):\n",
        "  def __init__(self,input_shape = None,**kwargs):\n",
        "    super().__init__(**kwargs)\n",
        "    self.Layer1 = tf.keras.layers.Dense(256,activation=tf.keras.activations.leaky_relu)\n",
        "    self.Layer2 = tf.keras.layers.Dense(128,activation=tf.keras.activations.leaky_relu)\n",
        "    self.Layer3 = tf.keras.layers.Dense(64,activation=tf.keras.activations.leaky_relu)\n",
        "    self.s_layer1 = tf.keras.layers.Dense(64)\n",
        "    self.s_layer2 = tf.keras.layers.Dense(64,activation=tf.keras.activations.leaky_relu)\n",
        "    self.output_layer = tf.keras.layers.Dense(2,activation=\"softmax\")\n",
        "    self.batch1_ = tf.keras.layers.BatchNormalization()\n",
        "    self.batch2_ = tf.keras.layers.BatchNormalization()\n",
        "    self.drop_ = tf.keras.layers.Dropout(0.3)\n",
        "\n",
        "  def call(self,X,training = False):\n",
        "    X = self.batch1_(X,training = training)\n",
        "    X = self.Layer1(X)\n",
        "    X = self.batch2_(X,training = training)\n",
        "    X = self.drop_(X,training = training)\n",
        "    X = self.Layer2(X)\n",
        "    X = self.Layer3(X)\n",
        "    X = self.drop_(X,training = training)\n",
        "\n",
        "    L1_star = self.s_layer1(X)\n",
        "    L2_star = self.s_layer2(L1_star)\n",
        "\n",
        "    combined_X = tf.concat([X,L2_star],axis=-1)\n",
        "\n",
        "    return self.output_layer(combined_X)\n",
        "\n",
        "\n",
        "early_stop = tf.keras.callbacks.EarlyStopping(\n",
        "    monitor='val_loss',\n",
        "    patience=6,\n",
        "    restore_best_weights=True\n",
        ")\n",
        "\n",
        "lr_decay = tf.keras.optimizers.schedules.ExponentialDecay(initial_learning_rate=0.01,decay_rate=0.01,decay_steps=7000)\n",
        "\n",
        "model = MyModel()\n",
        "opti = tf.keras.optimizers.Adam(learning_rate=lr_decay)\n",
        "model.compile(optimizer = opti, loss=\"sparse_categorical_crossentropy\", metrics=[\"accuracy\"])\n",
        "\n",
        "Xtrain,Xvalid = prepare_Data(df)\n",
        "\n",
        "model.fit(Xtrain,validation_data = Xvalid,\n",
        "    epochs=50,\n",
        "    batch_size=32,\n",
        "    callbacks=[early_stop],\n",
        "    class_weight = class_weight_dict\n",
        ")\n",
        "# Test anaylysis\n",
        "test_frame = pd.read_csv(\"/content/test.csv\")[:50000]\n",
        "test_frame = preprocess(test_frame)\n",
        "def make_test_data(frame):\n",
        "  X = frame.copy()\n",
        "  data = tf.data.Dataset.from_tensor_slices(dict(X)).batch(100).prefetch(1)\n",
        "  return data\n",
        "\n",
        "Xtest = make_test_data(test_frame)\n",
        "\n",
        "y_pred_probs = model.predict(Xtest)\n",
        "y_pred = np.argmax(y_pred_probs,axis = 1)\n",
        "plt.figure(figsize = (12,12))\n",
        "plt.subplot(1,2,1)\n",
        "sns.countplot(x = y_pred_probs)\n",
        "\n",
        "plt.subplot(1,2,2)\n",
        "sns.histplot(y_pred_probs[:,1], bins=30, kde=True)\n",
        "plt.show()"
      ]
    }
  ]
}